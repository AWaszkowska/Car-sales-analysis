{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "memory = '20g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "pyspark_submit_args = ' --executor-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d31c7",
   "metadata": {},
   "source": [
    "Creating a PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf48b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName('CarSales').getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .appName(\"CarSales\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files from hdfs\n",
    "rus25 = spark.read.csv(\"hdfs://localhost:9000/user/russia/region25_en.csv\", inferSchema=True, header=True)\n",
    "rus41 = spark.read.csv(\"hdfs://localhost:9000/user/russia/region41_en.csv\", inferSchema=True, header=True)\n",
    "usa = spark.read.csv(\"hdfs://localhost:9000/user/usa/us-dealers-used.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql as sparksql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e21d1",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus25.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus25.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f243622",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus25.select(\"engineDisplacement\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa.select(\"trim\").show(10)\n",
    "usa.select(\"engine_size\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be486a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus25.describe()\n",
    "rus41.describe()\n",
    "usa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame as a temporary view\n",
    "rus25.createOrReplaceTempView('rus25')\n",
    "rus41.createOrReplaceTempView('rus41')\n",
    "usa.createOrReplaceTempView('usa')\n",
    "\n",
    "# perform some queries to explore data\n",
    "spark.sql(\"SELECT brand, count(brand) as brand_count FROM rus25 GROUP BY brand ORDER BY brand_count DESC\").show()\n",
    "# spark.sql(\"SELECT bodyType, count(bodyType) as b_count, count(bodyType)*100/sum(count(bodyType)) over() as percent FROM rus25 GROUP BY bodyType\").show()\n",
    "spark.sql(\"SELECT year, bodyType, count(bodyType) as b_count, count(bodyType)*100/sum(count(bodyType)) over(PARTITION BY year) as percent FROM rus25 GROUP BY year, bodyType\").show()\n",
    "usa.filter((usa['make'] == 'Lexus') & (usa['price'] > 55000)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750aa4e",
   "metadata": {},
   "source": [
    "The second query was throwing a warning:\n",
    "23/06/02 22:04:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
    "\n",
    "Which means that the window operation does not have a defined partition, so all the data is being moved to a single partition. To enable distribution of the data across multiple partitions for parallel processing we can add a partitioning column, what I do in the 3rd query.\n",
    "\n",
    "Another insight is that the usa dataset does not contain a color feature, which can hugely impact ability of the model to predict the prices, because people are not rationally thinking creatures. If I was investigating this data for business analysis, I would ditch this dataset.\n",
    "\n",
    "Also, the value \"Truck\" in the vehicle_type column of the usa dataset seems to be disturbing, because obviously a car is not a truck, right? So I googled a few records just to find out that those are just vans, as described in \"body_type\" column. So we can drop \"vehicle_type\" column and not bother."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043cd5e",
   "metadata": {},
   "source": [
    "Here starts the data preprocessing part. Since the rus25 and rus41 dataframes have the same column names, I will merge them, because differentiating between regions is irrelevant for this work. Before doing so, I check if the schema is for sure the same, because union does not take care of it, neither removes duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab76425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "if rus25.schema == rus41.schema:\n",
    "    print(\"The schemas are the same\")\n",
    "else:\n",
    "    print(\"The schemas are different\")\n",
    "\n",
    "rus = rus25.union(rus41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ed747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for redundancy\n",
    "usa = usa.dropDuplicates()\n",
    "rus = rus.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377e8e7",
   "metadata": {},
   "source": [
    "First thing I need to do is dropping unuseful columns:\n",
    "a) USA:\n",
    "    id, vin, stock_no, street, city, state, zip, trim, vehicle_type\n",
    "b) Russia:\n",
    "    link, date, parse_date, location, vehicleConfiguration\n",
    "    \n",
    "Both datasets have their better and worse sides. Russian dataset is better because of \"color\" and \"power\" features, and USA dataset is better because of \"seller_name\" and \"drivetrain\" features. I decided to not remove those four distinct attributes, as they provide a meaningful business insight.\n",
    "\n",
    "Also, it's possible to make an early sociological conclusion based on the very structure of the dataset, that probably American people are more easily influenced by the selling person or company, meanwhile Russian people care more about technical aspects, such as power of the engine or color of a car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_usa = [\"id\", \"vin\", \"stock_no\", \"street\", \"city\", \"zip\", \"state\", \"trim\", \"vehicle_type\"]\n",
    "drop_cols_russ = [\"link\", \"date\", \"parse_date\", \"location\",\"vehicleConfiguration\"]\n",
    "\n",
    "usa = usa.drop(*drop_cols_usa)\n",
    "rus = rus.drop(*drop_cols_russ)\n",
    "\n",
    "usa.head(1)\n",
    "rus.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e75ca",
   "metadata": {},
   "source": [
    "The datasets from Russia and USA are quite different, but it's possible to make them more compatible, and overall better. \n",
    "\n",
    "Overall, the compatible columns are:\n",
    "- brand (rus) is make (us)\n",
    "- price is price, but I have to convert\n",
    "- mileage (rus) is miles (us), plus conversion needed\n",
    "- name (rus) is model (us)\n",
    "- fuelType (rus) is fuel_type (us), and there are some values that need further investigation\n",
    "- bodyType (rus) is body_type (us), and probably I will convert it to some more general terms\n",
    "- engineName (rus) is not compatible but I'll try to convert it to values similar to engine_block from usa DF, using chatgpt extension\n",
    "\n",
    "Moreover, it's still necessary to investigate those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brand/make\n",
    "usa.select(\"make\").distinct().show()\n",
    "rus.select(\"brand\").distinct().show()\n",
    "# as we can see, only the column name is different, but the way of naming brands is the same, so I just rename\n",
    "rus = rus.withColumnRenamed(\"brand\", \"make\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbff725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price will be convertet to euros in both cases\n",
    "from pyspark.sql.functions import round\n",
    "dolar_eur = 0.8770\n",
    "rubel_eur = 0.0122\n",
    "usa = usa.withColumn(\"price\", round(usa[\"price\"] * dolar_eur, 3))\n",
    "rus = rus.withColumn(\"price\", round(rus[\"price\"] * rubel_eur, 3))\n",
    "\n",
    "random_rows = rus.select(\"price\").sample(False, 0.1, seed=42).limit(5)\n",
    "random_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a623a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mileage\n",
    "usa = usa.withColumnRenamed(\"miles\", \"mileage\")\n",
    "mile_to_km = 1.609344\n",
    "usa = usa.withColumn(\"mileage\", round(usa[\"mileage\"] * mile_to_km, 1))\n",
    "\n",
    "random_rows = usa.select(\"mileage\").sample(False, 0.1, seed=42).limit(5)\n",
    "random_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb99aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuel type\n",
    "rus = rus.withColumnRenamed(\"fuelType\", \"fuel_type\")\n",
    "rus.select(\"fuel_type\").distinct().show()\n",
    "we can see that the only values are Gasoline, Diesel, Electro and null. \n",
    "these are relevant and simple, american DF has 43 unique values \n",
    "which is not informative, so I will convert relevant american values to russian, saving gas, methanol and hydrogen\n",
    "unique_values = usa.select(\"fuel_type\").distinct()\n",
    "unique_values.show(unique_values.count(), truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "usa = usa.withColumn(\"fuel_type\", when(col(\"fuel_type\").contains(\"Electric\"), \"Electro\")\n",
    "                                  .when(col(\"fuel_type\").contains(\"Biodiesel\"), \"Diesel\")\n",
    "                                  .when(col(\"fuel_type\").contains(\"E85\"), \"Gasoline\")\n",
    "                                  .when(col(\"fuel_type\").contains(\"Diesel\"), \"Diesel\")\n",
    "                                  .when(col(\"fuel_type\").contains(\"M85\"), \"Methanol\")\n",
    "                                   .when(col(\"fuel_type\").contains(\"Methanol\"), \"Methanol\")\n",
    "                                    .when(col(\"fuel_type\").contains(\"Unleaded\") & ~col(\"fuel_type\").contains(\"Diesel\"), \"Gasoline\")\n",
    "                                  .when(col(\"fuel_type\").contains(\"Gas\"), \"Gas\")\n",
    "                                  .otherwise(usa[\"fuel_type\"]))\n",
    "\n",
    "unique_values = usa.select(\"fuel_type\").distinct()\n",
    "unique_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7faedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rus = rus.withColumnRenamed(\"name\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bodyType\n",
    "from pyspark.sql.functions import col, when\n",
    "rus = rus.withColumnRenamed(\"bodyType\", \"body_type\")\n",
    "unique_values = rus.select(\"body_type\").distinct()\n",
    "unique_values.show(unique_values.count(), truncate=False)\n",
    "unique_values2 = usa.select(\"body_type\").distinct()\n",
    "unique_values2.show(unique_values2.count(), truncate=False)\n",
    "rus.createOrReplaceTempView('rus')\n",
    "spark.sql(\"SELECT make FROM rus WHERE body_type = 'jeep 3 doors'\").show()\n",
    "# as I supposed, \"jeep\" doesn't meen jeep, its a suv\n",
    "rus = rus.withColumn(\"body_type\", when(col(\"body_type\").contains(\"jeep\"), \"suv\")\n",
    "                     .when(col(\"body_type\").contains(\"hatchback\"), \"hatchback\")\n",
    "                     .when(col(\"body_type\").contains(\"wagon\"), \"combi\")\n",
    "                     .when(col(\"body_type\").contains(\"SUV\"), \"suv\")\n",
    "                     .when(col(\"body_type\").contains(\"liftback\"), \"sedan\")\n",
    "                     .otherwise(rus[\"body_type\"]))\n",
    "\n",
    "usa = usa.withColumn(\"body_type\", when(col(\"body_type\").contains(\"Sedan\"), \"sedan\")\n",
    "                     .when(col(\"body_type\").contains(\"Hatchback\"), \"hatchback\")\n",
    "                     .when(col(\"body_type\").contains(\"Targa\"), \"open\")\n",
    "                     .when(col(\"body_type\").contains(\"Crossover\"), \"suv\")\n",
    "                     .when(col(\"body_type\").contains(\"Chassis Cab\"), \"pickup\")\n",
    "                     .when(col(\"body_type\").contains(\"Convertible\"), \"open\")\n",
    "                     .when(col(\"body_type\").contains(\"Combi\"), \"combi\")\n",
    "                     .when(col(\"body_type\").contains(\"Pickup\"), \"pickup\")\n",
    "                     .when(col(\"body_type\").contains(\"Roadster\"), \"open\")\n",
    "                     .when(col(\"body_type\").contains(\"Wagon\"), \"combi\")\n",
    "                     .when(col(\"body_type\").contains(\"Commercial Wagon\"), \"combi\")\n",
    "                     .when(col(\"body_type\").contains(\"Minivan\"), \"minivan\")\n",
    "                     .when(col(\"body_type\").contains(\"Coupe\"), \"coupe\")\n",
    "                     .when(col(\"body_type\").contains(\"SUV\"), \"suv\")\n",
    "                     .otherwise(usa[\"body_type\"]))\n",
    "\n",
    "# drop cargo van, cutaway, micro car, van, passenger van, chassis cowl, mini mpv, car van\n",
    "drops = [\"Cargo Van\", \"Mini Mpv\", \"Van\", \"Car Van\", \"Cutaway\", \"Micro Car\", \"Cargo Van\", \"Passenger Van\"]\n",
    "usa = usa.filter(~col(\"body_type\").isin(drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b43a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineName to engine_block\n",
    "\n",
    "unique_values2 = usa.select(\"engine_block\").distinct()\n",
    "unique_values2.show(unique_values2.count(), truncate=False)\n",
    "\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99630fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \n",
    "\n",
    "def generate(user_input):\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=user_input,\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "rus = rus.withColumnRenamed(\"engineName\", \"engine_block\")\n",
    "uniqs = rus.select(\"engine_block\").distinct().collect()\n",
    "unilst = [row[\"engine_block\"] for row in uniqs]\n",
    "\n",
    "conversion_map = {\n",
    "    'V': 'V Engine',\n",
    "    'I': 'I Engine',\n",
    "    'H': 'H Engine'\n",
    "}\n",
    "\n",
    "converted_values = []\n",
    "for value in unilst:\n",
    "    user_input = f\"What is the engine block type for '{value}', you can choose V, I or H?\"\n",
    "    response = generate(user_input)\n",
    "    converted_value = conversion_map.get(response, 'Unknown')\n",
    "    converted_values.append(converted_value)\n",
    "\n",
    "# Create a DataFrame with the original and converted values\n",
    "data = {'Engine Name': unilst, 'Engine Block Type': converted_values}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f67b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine_size\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "rus = rus.withColumnRenamed(\"engineDisplacement\", \"engine_size\")\n",
    "rus = rus.withColumn('engine_size', regexp_extract(rus['engine_size'], r'(\\d+\\.\\d+|\\d+)', 1))\n",
    "\n",
    "# Show the updated column\n",
    "# rus.select('engine_size').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop engine name/block\n",
    "rus = rus.withColumnRenamed(\"engineName\", \"engine_block\")\n",
    "usa = usa.drop(\"engine_block\")\n",
    "rus = rus.drop(\"engine_block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e576e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage null values\n",
    "# first check how many of them\n",
    "from pyspark.sql.functions import col, sum, isnan\n",
    "nuls = usa.select([sum(col(c).isNull().cast(\"int\") + isnan(col(c)).cast(\"int\")).alias(c) for c in usa.columns])\n",
    "nuls.show()\n",
    "total = usa.count()\n",
    "print(total)\n",
    "\n",
    "nuls2 = rus.select([sum(col(c).isNull().cast(\"int\") + isnan(col(c)).cast(\"int\")).alias(c) for c in rus.columns])\n",
    "nuls2.show()\n",
    "total2 = rus.count()\n",
    "print(total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where price, drivetrain, transmission, power, seller name and fuel type is missing\n",
    "dropus = ['price', 'drivetrain', 'transmission', 'seller_name', 'fuel_type', 'engine_size']\n",
    "dropru = ['price', 'transmission', 'power', 'fuel_type', 'engine_size']\n",
    "rus = rus.dropna(subset=dropru)\n",
    "usa = usa.dropna(subset=dropus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf74bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year: inpute with mean year of cars with mileage same +- 10000\n",
    "from pyspark.sql.functions import avg, when, col, coalesce\n",
    "\n",
    "average_year = usa.groupBy(((col(\"mileage\") / 10000).cast(\"integer\") * 10000).alias(\"mileage_range\")) \\\n",
    "                 .agg(avg(\"year\").cast(\"integer\").alias(\"average_year\"))\n",
    "\n",
    "usa = usa.join(average_year, ((usa[\"mileage\"] / 10000).cast(\"integer\") * 10000) == col(\"mileage_range\"), \"left\")\n",
    "usa = usa.withColumn(\"year\", when(col(\"year\").isNull(), col(\"average_year\")).otherwise(col(\"year\")))\n",
    "usa = usa.drop(\"mileage_range\", \"average_year\")\n",
    "\n",
    "usa = usa.dropna(subset=[\"year\"], how=\"any\")\n",
    "\n",
    "average_rok = rus.groupBy(((col(\"mileage\") / 10000).cast(\"integer\") * 10000).alias(\"mileage_range_rus\")) \\\n",
    "                 .agg(avg(\"year\").cast(\"integer\").alias(\"average_rok\"))\n",
    "\n",
    "rus = rus.join(average_rok, ((rus[\"mileage\"] / 10000).cast(\"integer\") * 10000) == col(\"mileage_range_rus\"), \"left\")\n",
    "rus = rus.withColumn(\"year\", when(col(\"year\").isNull(), col(\"average_rok\")).otherwise(col(\"year\")))\n",
    "rus = rus.drop(\"mileage_range_rus\", \"average_rok\")\n",
    "\n",
    "nuls = rus.select([sum(col(c).isNull().cast(\"int\") + isnan(col(c)).cast(\"int\")).alias(c) for c in rus.columns])\n",
    "nuls.show()\n",
    "\n",
    "rus = rus.dropna(subset=[\"year\"], how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cb3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color: inpute randomly within colors list\n",
    "from pyspark.sql.functions import array, lit, rand\n",
    "\n",
    "colors = rus.select(\"color\").distinct().collect()\n",
    "c_list = [row[\"color\"] for row in colors]\n",
    "\n",
    "rus = rus.withColumn(\"color\", array([lit(c) for c in c_list])[(rand() * len(c_list)).cast(\"int\")])\n",
    "# rus.select(\"color\").show()\n",
    "rus = rus.dropna(subset=[\"color\"], how=\"any\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337eb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mileage: inpute with average mileage of cars from same year\n",
    "# coalesce() func is used to return the first non-null val among specified cols\n",
    "average_mileage_rus = rus.groupBy(\"year\").agg(avg(\"mileage\").alias(\"average_mileage_rus\"))\n",
    "rus = rus.join(average_mileage_rus, \"year\", \"left\")\n",
    "rus = rus.withColumn(\"mileage\", coalesce(col(\"mileage\"), col(\"average_mileage_rus\")))\n",
    "rus = rus.drop(\"average_mileage_rus\")\n",
    "\n",
    "nuls = rus.select([sum(col(c).isNull().cast(\"int\") + isnan(col(c)).cast(\"int\")).alias(c) for c in rus.columns])\n",
    "# nuls.show()\n",
    "# as we can see, we still get 57 records with missing data, where it wasn't possible to infer the value from years\n",
    "# I drop them\n",
    "rus = rus.dropna(subset=[\"mileage\"], how=\"any\")\n",
    "\n",
    "average_mileage_us = usa.groupBy(\"year\").agg(avg(\"mileage\").alias(\"average_mileage_us\"))\n",
    "usa = usa.join(average_mileage_us, \"year\", \"left\")\n",
    "usa = usa.withColumn(\"mileage\", coalesce(col(\"mileage\"), col(\"average_mileage_us\")))\n",
    "usa = usa.drop(\"average_mileage_us\")\n",
    "\n",
    "nuls = usa.select([sum(col(c).isNull().cast(\"int\") + isnan(col(c)).cast(\"int\")).alias(c) for c in usa.columns])\n",
    "# nuls.show()\n",
    "# no need to drop anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b307da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36f13a",
   "metadata": {},
   "source": [
    "What is the most common brand, color, fuel type during years, for USA and Russia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31815475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "make_counts = rus.groupBy(\"year\", \"make\").count().orderBy(\"year\", desc(\"count\"))\n",
    "color_counts = rus.groupBy(\"year\", \"color\").count().orderBy(\"year\", desc(\"count\"))\n",
    "fuel_type_counts = rus.groupBy(\"year\", \"fuel_type\").count().orderBy(\"year\", desc(\"count\"))\n",
    "\n",
    "make_counts_pd = make_counts.toPandas()\n",
    "color_counts_pd = color_counts.toPandas()\n",
    "fuel_type_counts_pd = fuel_type_counts.toPandas()\n",
    "\n",
    "Get the top 10 most popular makes\n",
    "top_10_makes = make_counts_pd.groupby('make').sum().nlargest(10, 'count').index\n",
    "make_counts_pd_filtered = make_counts_pd[make_counts_pd['make'].isin(top_10_makes)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.barplot(data=make_counts_pd_filtered, x=\"year\", y=\"count\", hue=\"make\", palette='tab10')\n",
    "plt.title(\"Most Common Brand during Years - Russia\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.barplot(data=color_counts_pd, x=\"year\", y=\"count\", hue=\"color\")\n",
    "plt.title(\"Most Common Color during Years - Russia\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.barplot(data=fuel_type_counts_pd, x=\"year\", y=\"count\", hue=\"fuel_type\")\n",
    "plt.title(\"Most Common Fuel Type during Years - Russia\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "usa_make_counts = usa.groupBy(\"year\", \"make\").count().orderBy(\"year\", desc(\"count\"))\n",
    "usa_fuel_type_counts = usa.groupBy(\"year\", \"fuel_type\").count().orderBy(\"year\", desc(\"count\"))\n",
    "\n",
    "usa_make_counts_pd = usa_make_counts.toPandas()\n",
    "usa_fuel_type_counts_pd = usa_fuel_type_counts.toPandas()\n",
    "\n",
    "# Get the top 10 most popular makes\n",
    "usa_top_10_makes = usa_make_counts_pd.groupby('make').sum().nlargest(10, 'count').index\n",
    "usa_make_counts_pd_filtered = usa_make_counts_pd[usa_make_counts_pd['make'].isin(usa_top_10_makes)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(data=usa_make_counts_pd_filtered, x=\"year\", y=\"count\", hue=\"make\", palette='tab10')\n",
    "plt.title(\"Most Common Brand during Years - USA\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(data=usa_fuel_type_counts_pd, x=\"year\", y=\"count\", hue=\"fuel_type\")\n",
    "plt.title(\"Most Common Fuel Type during Years - USA\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c815a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data follow normal distrib\n",
    "import matplotlib.pyplot as plt\n",
    "cols = [\"year\", \"mileage\", \"power\", \"price\", \"engine_size\", \"make\", \"model\", \"body_type\", \"color\", \"fuel_type\", \"transmission\"]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms - Russia\")\n",
    "for i, col_name in enumerate(cols):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    data = rus.select(col(col_name)).toPandas()\n",
    "    plt.figure()\n",
    "    plt.hist(data[col_name], bins=20)\n",
    "    plt.title(col_name)\n",
    "    plt.xlabel(col_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms - USA\")\n",
    "colu = [\"year\", \"mileage\", \"price\", \"engine_size\", \"make\", \"model\", \"body_type\", \"fuel_type\", \"transmission\", \"seller_name\", \"drivetrain\"]\n",
    "for i, col_name in enumerate(colu):\n",
    "    plt.subplot(3, 4, i+1) \n",
    "    data = usa.select(col(col_name)).toPandas()\n",
    "    plt.figure()\n",
    "    plt.hist(data[col_name], bins=20)\n",
    "    plt.title(col_name)\n",
    "    plt.xlabel(col_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5ba0a",
   "metadata": {},
   "source": [
    "Median Car Price by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_median_price = rus.groupBy(\"year\").agg({\"price\": \"median\"}).orderBy(\"year\")\n",
    "rus_median_price_pd = rus_median_price.toPandas()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "usa_median_price = usa.groupBy(\"year\").agg({\"price\": \"median\"}).orderBy(\"year\")\n",
    "usa_median_price_pd = usa_median_price.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=rus_median_price_pd, x=\"year\", y=\"median(price)\")\n",
    "plt.title(\"Median Car Price by Year - Russia\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=usa_median_price_pd, x=\"year\", y=\"median(price)\")\n",
    "plt.title(\"Median Car Price by Year - USA\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae6430",
   "metadata": {},
   "source": [
    "Average prices of cars by make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import avg\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "top_brands = rus.groupBy(\"make\").count().orderBy(\"count\", ascending=False).limit(3).select(\"make\").rdd.flatMap(lambda x: x).collect()\n",
    "brand_prices = rus.filter(rus[\"make\"].isin(top_brands)).groupBy([\"make\", \"year\"]).agg(avg(\"price\").alias(\"average_price\")).orderBy(\"make\", \"year\").toPandas()\n",
    "\n",
    "years = brand_prices[\"year\"].unique()\n",
    "years = years[::5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.barplot(data=brand_prices, x=\"year\", y=\"average_price\", hue=\"make\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Average Car Prices by Make - Russia\")\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import avg\n",
    "# Group by fuel type and calculate average price for each year\n",
    "fuel_type_prices = rus.groupby([\"fuel_type\", \"year\"]).agg(avg(\"price\").alias(\"average_price\")).orderBy(\"fuel_type\", \"year\").toPandas()\n",
    "\n",
    "fuel_types = fuel_type_prices[\"fuel_type\"].unique()\n",
    "\n",
    "years = fuel_type_prices[\"year\"].unique()\n",
    "years = years[::5]\n",
    "\n",
    "n_fuel_types = len(fuel_types)\n",
    "bar_width = 0.8 / n_fuel_types\n",
    "opacity = 0.8\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, fuel_type in enumerate(fuel_types):\n",
    "    data = fuel_type_prices[fuel_type_prices[\"fuel_type\"] == fuel_type]\n",
    "    plt.bar(data[\"year\"], data[\"average_price\"], width=bar_width, alpha=opacity, label=fuel_type)\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Average Car Prices by Fuel Type - Russia\")\n",
    "plt.xticks(years)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fuel_type_prices = usa.groupby([\"fuel_type\", \"year\"]).agg(avg(\"price\").alias(\"average_price\")).orderBy(\"fuel_type\", \"year\").toPandas()\n",
    "\n",
    "fuel_types = fuel_type_prices[\"fuel_type\"].unique()\n",
    "\n",
    "years = fuel_type_prices[\"year\"].unique()\n",
    "years = years[::5]\n",
    "\n",
    "n_fuel_types = len(fuel_types)\n",
    "bar_width = 0.8 / n_fuel_types\n",
    "opacity = 0.8\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, fuel_type in enumerate(fuel_types):\n",
    "    data = fuel_type_prices[fuel_type_prices[\"fuel_type\"] == fuel_type]\n",
    "    plt.bar(data[\"year\"], data[\"average_price\"], width=bar_width, alpha=opacity, label=fuel_type)\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Average Car Prices by Fuel Type - USA\")\n",
    "plt.xticks(years)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5e8c3",
   "metadata": {},
   "source": [
    "Average price for specific fuel type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after analysis, dropping columns that are different\n",
    "usa = usa.drop(\"seller_name\", \"drivetrain\")\n",
    "rus = rus.drop(\"power\", \"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8996bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f753b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding and handling data types\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "numeric_cols = [\"year\", \"mileage\",\"price\", \"engine_size\"]\n",
    "for col_name in numeric_cols:\n",
    "    rus = rus.withColumn(col_name, col(col_name).cast(\"float\"))\n",
    "rus = rus.filter(col(\"year\") > 2010)\n",
    "   \n",
    "cat_cols = [\"make\", \"model\", \"body_type\", \"fuel_type\", \"transmission\"]\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+\"_index\") for col_name in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[col_name+\"_index\"], outputCols=[col_name+\"_encoded\"]) for col_name in cat_cols]\n",
    "\n",
    "for indexer in indexers:\n",
    "    rus = indexer.fit(rus).transform(rus)\n",
    "\n",
    "for encoder in encoders:\n",
    "    rus = encoder.fit(rus).transform(rus)\n",
    "    \n",
    "columns_to_drop = cat_cols + [col_name+\"_index\" for col_name in cat_cols]\n",
    "rus = rus.drop(*columns_to_drop)\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    usa = usa.withColumn(col_name, col(col_name).cast(\"float\"))\n",
    "\n",
    "indexerss = [StringIndexer(inputCol=col_name, outputCol=col_name+\"_index\") for col_name in cat_cols]\n",
    "encoderss = [OneHotEncoder(inputCols=[col_name+\"_index\"], outputCols=[col_name+\"_encoded\"]) for col_name in cat_cols]\n",
    "\n",
    "for indexer in indexerss:\n",
    "    usa = indexer.fit(usa).transform(usa)\n",
    "\n",
    "for encoder in encoderss:\n",
    "    usa = encoder.fit(usa).transform(usa)\n",
    "    \n",
    "cols_to_drop = cat_cols + [col_name+\"_index\" for col_name in cat_cols]\n",
    "usa = usa.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04eae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "rus_train, rus_test = rus.randomSplit([0.8, 0.2], seed=42)\n",
    "usa_train, usa_test = usa.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = [\"year\", \"mileage\", \"engine_size\", \"make_encoded\", \"model_encoded\", \"body_type_encoded\", \"fuel_type_encoded\", \"transmission_encoded\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "rus_train = assembler.transform(rus_train)\n",
    "# usa_train = assembler.transform(usa_train)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "rus_lr_model = lr.fit(rus_train)\n",
    "usa_lr_model = lr.fit(usa_train)\n",
    "\n",
    "# predictions\n",
    "rus_predictions = rus_lr_model.transform(assembler.transform(rus_test))\n",
    "usa_predictions = usa_lr_model.transform(assembler.transform(usa_test))\n",
    "\n",
    "# MAE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", metricName=\"mae\")\n",
    "rus_mae = evaluator.evaluate(rus_predictions)\n",
    "# usa_mae = evaluator.evaluate(usa_predictions)\n",
    "\n",
    "print(\"MAE for rus model:\", rus_mae)\n",
    "# print(\"MAE for usa model:\", usa_mae)\n",
    "\n",
    "# RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "rus_rmse = evaluator.evaluate(rus_predictions)\n",
    "# usa_rmse = evaluator.evaluate(usa_predictions)\n",
    "\n",
    "print(\"RMSE for rus model:\", rus_rmse)\n",
    "# print(\"RMSE for usa model:\", usa_rmse)\n",
    "\n",
    "# loss function\n",
    "loss_val = rus_lr_model.summary.losses\n",
    "figure()\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function - Russia')\n",
    "plt.show()\n",
    "\n",
    "loss_val = usa_lr_model.summary.losses\n",
    "figure()\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function - USA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "input_cols = [\"year\", \"mileage\", \"engine_size\", \"make_encoded\", \"model_encoded\", \"body_type_encoded\", \"fuel_type_encoded\", \"transmission_encoded\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "rus_train = assembler.transform(rus_train)\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
    "rus_rf_model = rf.fit(rus_train)\n",
    "# usa_rf_model = rf.fit(usa_train)\n",
    "\n",
    "#predictions\n",
    "rus_predictions = rus_rf_model.transform(assembler.transform(rus_test))\n",
    "usa_predictions = usa_rf_model.transform(assembler.transform(usa_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ab415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# MAE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", metricName=\"mae\")\n",
    "\n",
    "rus_mae = evaluator.evaluate(rus_predictions)\n",
    "usa_mae = evaluator.evaluate(usa_rf_predictions)\n",
    "\n",
    "print(\"MAE for rus Random Forest model:\", rus_mae)\n",
    "print(\"MAE for usa Random Forest model:\", usa_mae)\n",
    "\n",
    "#RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "rus_rmse = evaluator.evaluate(rus_predictions)\n",
    "usa_rmse = evaluator.evaluate(usa_rf_predictions)\n",
    "\n",
    "print(\"RMSE for rus Random Forest model:\", rus_rmse)\n",
    "print(\"RMSE for usa Random Forest model:\", usa_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9847ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
